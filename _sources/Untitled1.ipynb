{"cells":[{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5828,"status":"ok","timestamp":1684377422776,"user":{"displayName":"TEGAR FAKHRUDDIN","userId":"06726564812162352317"},"user_tz":-420},"id":"WnZvK6zcrLP_","outputId":"74354d82-bb55-4611-83a2-10769d003ce9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scrapy in /usr/local/lib/python3.10/dist-packages (2.9.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: Twisted>=18.9.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (22.10.0)\n","Requirement already satisfied: cryptography>=3.4.6 in /usr/local/lib/python3.10/dist-packages (from scrapy) (40.0.2)\n","Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.2.0)\n","Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.1.0)\n","Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.8.1)\n","Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.1.1)\n","Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.6.2)\n","Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (21.1.0)\n","Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.1.1)\n","Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (6.0)\n","Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.2.1)\n","Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.1)\n","Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from scrapy) (3.4.2)\n","Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.2)\n","Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.0.7)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4.6->scrapy) (1.15.1)\n","Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from protego>=0.1.15->scrapy) (1.16.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.0)\n","Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n","Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n","Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n","Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.5.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.4)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.27.1)\n","Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (1.5.1)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.12.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.21)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.12)\n","2023-05-18 02:37:10 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: scrapybot)\n","2023-05-18 02:37:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.15.107+-x86_64-with-glibc2.31\n","Usage\n","=====\n","  scrapy runspider [options] <spider_file>\n","runspider: error: File not found: scrapy_spider.py\n","\n"]}],"source":["!pip install scrapy pandas\n","! scrapy runspider scrapy_spider.py"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":54,"status":"ok","timestamp":1684377422777,"user":{"displayName":"TEGAR FAKHRUDDIN","userId":"06726564812162352317"},"user_tz":-420},"id":"XldXe9WHrArM"},"outputs":[],"source":["import scrapy\n","from scrapy.crawler import CrawlerProcess\n","import pandas as pd"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"rFLZiC4zrDOi","executionInfo":{"status":"error","timestamp":1684377422778,"user_tz":-420,"elapsed":51,"user":{"displayName":"TEGAR FAKHRUDDIN","userId":"06726564812162352317"}},"outputId":"659cbdb4-b5b6-42f1-90f8-33ff643adf91","colab":{"base_uri":"https://localhost:8080/","height":1000}},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:scrapy.utils.log:Scrapy 2.9.0 started (bot: scrapybot)\n","2023-05-18 02:37:10 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: scrapybot)\n","INFO:scrapy.utils.log:Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.15.107+-x86_64-with-glibc2.31\n","2023-05-18 02:37:10 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform Linux-5.15.107+-x86_64-with-glibc2.31\n","INFO:scrapy.crawler:Overridden settings:\n","{}\n","2023-05-18 02:37:10 [scrapy.crawler] INFO: Overridden settings:\n","{}\n","DEBUG:scrapy.utils.log:Using reactor: twisted.internet.epollreactor.EPollReactor\n","2023-05-18 02:37:10 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n","INFO:scrapy.extensions.telnet:Telnet Password: 8e330ebf0b5576a8\n","2023-05-18 02:37:10 [scrapy.extensions.telnet] INFO: Telnet Password: 8e330ebf0b5576a8\n","INFO:scrapy.middleware:Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2023-05-18 02:37:10 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","INFO:scrapy.middleware:Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2023-05-18 02:37:10 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","INFO:scrapy.middleware:Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2023-05-18 02:37:10 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","INFO:scrapy.middleware:Enabled item pipelines:\n","[]\n","2023-05-18 02:37:10 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","INFO:scrapy.core.engine:Spider opened\n","2023-05-18 02:37:10 [scrapy.core.engine] INFO: Spider opened\n","INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2023-05-18 02:37:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6037\n","2023-05-18 02:37:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6037\n"]},{"output_type":"error","ename":"ReactorNotRestartable","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-b3078fd35753>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMySpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"REACTOR_THREADPOOL_MAXSIZE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"before\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shutdown\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_graceful_stop_reactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \"\"\"\n\u001b[1;32m   1298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mReactorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReactorBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reallyStartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mReactorNotRestartable\u001b[0m: "]}],"source":["class MySpider(scrapy.Spider): name = 'my_spider' \n","start_urls = ['https://play.google.com/store/apps/details?id=com.lazada.android&hl=id&gl=US']\n","def parse(self, response):\n","    # Contoh: Mendapatkan judul halaman web\n","    title = response.css('title::text').get()\n","    print(\"Judul halaman web:\", title)\n","    # Contoh: Mendapatkan semua tautan pada halaman web\n","    links = response.css('a::attr(href)').getall()\n","    print(\"Tautan pada halaman web:\")\n","    for link in links:\n","        print(link)\n","    # Menyimpan data ke file CSV\n","    csv_data = [{'Link': link} for link in links]\n","    df = pd.DataFrame(csv_data)\n","    csv_filename = \"Scrapy.csv\"\n","    df.to_csv(csv_filename, index=False)\n","    print(\"Data telah disimpan ke file CSV:\", csv_filename)\n","    # Menyimpan data ke file Excel (XLS)\n","    xls_filename = \"Scrapy.xls\"\n","    df.to_excel(xls_filename, index=False)\n","    print(\"Data telah disimpan ke file Excel (XLS):\",\n","    xls_filename)\n","# Menjalankan spider menggunakan CrawlerProcess\n","process = CrawlerProcess()\n","process.crawl(MySpider)\n","process.start();"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTbRXqoqjC32d2v4zrt4+M"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}